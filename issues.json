[{"body":"## Overview\nIntroduce a modern bundler (Vite or Webpack) for the frontend codebase to improve developer experience, enable ES modules, and optimize production builds.\n\n## Background\nIssue #21 refactors the frontend to use `api-client.js` with a \"bundle-ready\" structure (no circular dependencies, clear entry points). This issue builds on that foundation to introduce proper bundling.\n\n## Benefits\n\n### Developer Experience\n- Hot module replacement (HMR)\n- Source maps for debugging\n- TypeScript support (optional)\n- Import/export syntax\n\n### Production\n- Tree-shaking (remove unused code)\n- Minification\n- Code splitting\n- Cache busting\n\n## Proposed Approach: Vite\n\nVite is recommended over Webpack because:\n- Faster development server (native ES modules)\n- Simpler configuration\n- Built-in support for common patterns\n- Easy migration from script tags\n\n## Scope\n\n### Setup\n- [ ] Add `vite` and dependencies to package.json\n- [ ] Create `vite.config.js`\n- [ ] Configure build output to match current `public/` structure\n\n### Migration\n- [ ] Convert `config.js` and `api-client.js` to ES modules\n- [ ] Update HTML files to use module script tags\n- [ ] Update deployment scripts\n\n### Verification\n- [ ] Development server works\n- [ ] Production build works\n- [ ] All existing functionality preserved\n\n## Dependencies\n- Depends on #21 (Electron dependency refactor)\n- Optional dependency on #27 (folder consolidation)\n\n## Notes\nThis is a non-urgent enhancement. The current script tag approach works fine; this adds optimization and DX improvements.","labels":[{"id":"LA_kwDOQyPtOs8AAAACT1HZqg","name":"enhancement","description":"New feature or request","color":"a2eeef"}],"number":28,"title":"Introduce Vite/Webpack bundler for frontend assets"},{"body":"## Overview\nConsolidate the duplicate frontend codebases in `/shared/public/` and `/web-launcher/public/` to eliminate code duplication and maintenance burden.\n\n## Background\nCurrently there are two nearly-identical frontend codebases:\n- `/web-launcher/public/` - Used by the web server (primary)\n- `/shared/public/` - Used by electron-launcher and as fallback for web-launcher root route\n\nThis was discovered during Issue #21 planning. After #21 completes, `web-launcher/public/` will use the new `api-client.js` pattern while `shared/public/` will still use the old `preload.js` pattern.\n\n## Scope\n\n### Analysis Needed\n- [ ] Determine which version should be canonical\n- [ ] Identify differences between the two folders\n- [ ] Understand electron-launcher's requirements\n\n### Implementation Options\n1. **Symlink approach**: One canonical folder, electron-launcher uses symlink\n2. **Build-time copy**: Single source, copied during build\n3. **Merge and delete**: Consolidate into one location\n\n### Files to Reconcile\n- 10+ HTML files\n- preload.js vs api-client.js pattern\n- CSS and asset files\n\n## Dependencies\n- Depends on #21 (Electron dependency refactor for web-launcher)\n\n## Acceptance Criteria\n1. Single source of truth for frontend files\n2. Both web-launcher and electron-launcher function correctly\n3. No duplicate maintenance burden","labels":[{"id":"LA_kwDOQyPtOs8AAAACT1HZqg","name":"enhancement","description":"New feature or request","color":"a2eeef"}],"number":27,"title":"Consolidate shared/public and web-launcher/public folders"},{"body":"## Overview\nEnable the API layer to be deployed and scaled independently from the frontend static assets.\n\n## Background\nThis issue is a follow-up to #21 (Refactor Frontend to Remove Electron Dependency). Issue #21 will establish the **foundation** for API scalability by:\n- Creating a configurable API base URL in `api-client.js`\n- Using proper fetch patterns with credentials handling\n- Clean module separation\n\nThis issue handles the **infrastructure and backend changes** needed to actually deploy the API separately.\n\n## Scope\n\n### Backend Changes\n- [ ] Add CORS headers to all API endpoints\n- [ ] Configure allowed origins (environment-based)\n- [ ] Ensure authentication works cross-origin (cookies with SameSite or JWT tokens)\n\n### Infrastructure Changes\n- [ ] Create separate DigitalOcean App Platform service for API\n- [ ] Configure CDN/static hosting for frontend (or keep in same service initially)\n- [ ] Environment configuration for API_BASE_URL per environment\n- [ ] Update deployment scripts/app spec\n\n### Configuration\n- [ ] Environment variable: `API_BASE_URL`\n- [ ] CORS allowed origins configuration\n- [ ] Document deployment architecture\n\n## Acceptance Criteria\n1. Frontend can be served from a different origin than the API\n2. API can scale horizontally (multiple instances)\n3. Authentication continues to work correctly\n4. All existing functionality preserved\n\n## Dependencies\n- Depends on #21 (foundation work)\n\n## Notes\nThis enables future architecture where:\n- **Frontend** â†’ CDN/static hosting (fast, global)\n- **API** â†’ Horizontally scalable backend service (DigitalOcean App Platform with auto-scaling)","labels":[{"id":"LA_kwDOQyPtOs8AAAACT1HZqg","name":"enhancement","description":"New feature or request","color":"a2eeef"}],"number":26,"title":"API Layer Scalability: Separate Backend for Horizontal Scaling"},{"body":"## Summary\n\nCreate a demo data loader that populates the database with realistic sample data for testing and demonstration purposes.\n\n## Problem\n\nCurrently, testing the application requires manually creating customers, locations, services, bookings, etc. through the UI. This is time-consuming and makes it difficult to:\n- Test features with realistic data volumes\n- Demonstrate the application to stakeholders\n- Reset to a known state after testing\n- Onboard new developers quickly\n\n## Requirements\n\n### Data to Generate\n- [ ] Customers (10-20 with varied attributes)\n- [ ] Locations (2-5 per customer, mix of types)\n- [ ] Services (5-10 service offerings)\n- [ ] Vehicles (3-5 with different statuses)\n- [ ] Drivers (3-5 with vehicle assignments)\n- [ ] Bookings (50-100 across different dates, statuses, customers)\n- [ ] Routes (5-10 with assigned bookings)\n\n### Functional Requirements\n- [ ] Idempotent - can run multiple times without duplicating data\n- [ ] Configurable - ability to specify data volume (small/medium/large)\n- [ ] Realistic - data should look like real business data (real addresses, realistic names)\n- [ ] Relationships - properly linked foreign keys across all entities\n\n### Technical Requirements\n- [ ] CLI script that can be run via `npm run seed:demo` or similar\n- [ ] Uses existing service layer (not direct SQL)\n- [ ] Works with the `fleetillo` schema\n- [ ] Clears existing demo data before re-seeding (optional flag)\n\n## Acceptance Criteria\n\n1. Running the loader populates the database with demo data\n2. All UI screens show realistic data after loading\n3. Bookings appear on calendar views\n4. Routes can be viewed and optimized\n5. Data can be cleared and reloaded cleanly\n\n## Notes\n\n- Consider using a library like Faker.js for generating realistic data\n- May want to use real addresses for geolocation testing\n- Should support both development and staging environments","labels":[{"id":"LA_kwDOQyPtOs8AAAACVSanYg","name":"plan ready","description":"Planning complete - ready for implementation","color":"0052CC"},{"id":"LA_kwDOQyPtOs8AAAACVTMrpw","name":"plan: medium","description":"Medium (3-6 pts) - needs plan.md document","color":"FBCA04"}],"number":25,"title":"Build demo data loader for testing"},{"body":"## Summary\n\nEnable deployment of the Fleetillo database schema to different schema names, supporting multi-tenant or environment-specific deployments.\n\n## Background\n\nDuring the Issue #14 migration (clients â†’ customers, routeiq â†’ fleetillo), we identified that:\n- The migration script has hardcoded schema name ('fleetillo')\n- Grants need to be applied manually after schema creation\n- No automated validation of schema structure exists\n\n## Requirements\n\n### Deployment Script\n- [ ] Create parameterized schema deployment script\n- [ ] Accept schema name as parameter (default: 'fleetillo')\n- [ ] Automatically apply necessary grants for Supabase roles (anon, authenticated, service_role)\n- [ ] Include post-deployment validation step\n\n### Migration Support\n- [ ] Template the consolidated migration script to accept schema name variable\n- [ ] Document how to deploy to a new schema\n\n### Validation\n- [ ] Integrate schema validation into deployment process\n- [ ] Verify all tables, columns, and constraints exist\n- [ ] Verify RLS policies are applied\n\n## Example Usage\n\n```bash\n# Deploy to default schema\n./scripts/deploy-schema.sh\n\n# Deploy to custom schema name\n./scripts/deploy-schema.sh --schema customer_abc\n\n# Validate existing schema\n./scripts/deploy-schema.sh --validate-only --schema fleetillo\n```\n\n## Dependencies\n\n- Blocked by: Issue #14 (clients â†’ customers migration)\n\n## Notes\n\n- Current workaround: manually edit schema name in migration SQL and run grants separately\n- Validation script exists at `scripts/validate-fleetillo-schema.ts` (can be adapted)\n","labels":[{"id":"LA_kwDOQyPtOs8AAAACVSanYg","name":"plan ready","description":"Planning complete - ready for implementation","color":"0052CC"},{"id":"LA_kwDOQyPtOs8AAAACVTMrpw","name":"plan: medium","description":"Medium (3-6 pts) - needs plan.md document","color":"FBCA04"}],"number":23,"title":"Add parameterized schema deployment support"},{"body":"There are several files left over from the original build that used YokeFlow. YokeFlow is no longer being used for development of this application.\n\nWe need to clean these up carefully to avoid breaking anything.\n\n**Identified files/directories:**\n- `docs/yokeflow/` directory and its contents (9 files including `architecture_overview.md`, `optiroute_schema.sql`, etc.)\n- Reference in `docs/optiroute_code_specification.md`\n\n**Tasks:**\n- Remove `docs/yokeflow/` directory.\n- Search for and remove any other 'yokeflow' references.\n- Verify that removing these documentation files does not impact the build or dependencies.","labels":[{"id":"LA_kwDOQyPtOs8AAAACVTPbYw","name":"plan: simple","description":"Simple - no plan needed, ready to implement","color":"0E8A16"}],"number":22,"title":"Cleanup Legacy YokeFlow Files"},{"body":"The web-launcher frontend currently relies heavily on `window.electronAPI` for data fetching and operations. Since the Electron launcher is being deprecated, we need to decouple the frontend from this dependency and standardize on a web-based API client.\n\n**Scope of Work:**\n\n1.  **Create a Unified API Client:** Develop a `api-client.js` (or extend `dispatch-client.js`) that abstracts API calls. It should handle the logic of calling the backend API (likely via the existing `/api/rpc` endpoint or REST endpoints) without relying on injected Electron globals.\n2.  **Refactor Frontend Pages:** Systematically update all HTML/JS files to use this new API client instead of `window.electronAPI`. Affected files include:\n    *   `customers.html`\n    *   `services.html`\n    *   `settings.html`\n    *   `locations.html`\n    *   `vehicles.html`\n    *   `bookings.html`\n    *   `drivers.html`\n    *   `routes.html`\n    *   `dispatch.html` (cleanup existing fallback logic)\n3.  **Cleanup:** Remove Electron-specific preload scripts, IPC handlers, and conditional logic checks for `window.electronAPI`.\n\n**Goal:** Ensure the entire web-launcher functions correctly as a standard web application running in a browser.\n\n---\n\n## Update: API Scalability Foundation\n\nAs part of this refactor, we will include foundation work to support future API scalability (see #26):\n\n- **Configurable API base URL** in `api-client.js` (read from config/environment)\n- **Proper fetch patterns** with credentials handling for cross-origin readiness\n- **Clean module structure** that can easily be pointed at a separate API origin\n\nThis ensures the refactor not only removes Electron dependencies but also prepares the codebase for horizontal scaling of the API layer.","labels":[{"id":"LA_kwDOQyPtOs8AAAACT1HZqg","name":"enhancement","description":"New feature or request","color":"a2eeef"},{"id":"LA_kwDOQyPtOs8AAAACVSanYg","name":"plan ready","description":"Planning complete - ready for implementation","color":"0052CC"},{"id":"LA_kwDOQyPtOs8AAAACVTMrzw","name":"plan: complex","description":"Complex (7+ pts) - needs requirements.md, design.md, tasks.md","color":"D93F0B"}],"number":21,"title":"Refactor Frontend to Remove Electron Dependency"},{"body":"## Goal\nIdempotently load parsed CSV data into the database.\n\n## Source Data Summary\n- **Customers (Brands)**: 278 records (e.g., \"Applebee's\", \"Bojangles\")\n- **Locations (Sites)**: 870 records\n- **Bookings**: 1,193 service records\n\n## Tasks\n\n### 1. Order of Operations\n```\n1. Customers - 278 brand records (e.g., \"Applebee's\", \"Bojangles\")\n   â†“\n2. Locations - 870 site records, each linked to ONE customer via customer_id\n   â†“\n3. Bookings - 1,193 service records, each linked to ONE location via location_id\n```\n\n**Relationship Enforcement:**\n- Each Location MUST have a valid `customer_id` (FK to customers)\n- Each Booking MUST have a valid `location_id` (FK to locations)\n- Never create duplicate customers for the same brand\n- Never create duplicate locations for the same customer+site\n\n### 2. Idempotency Strategy\n- [ ] **Customer Matching**: By normalized name (e.g., \"APPLEBEES\")\n- [ ] **Location Matching**:\n  - Primary: `customer_id` + `address_hash` (SHA256 of normalized address)\n  - Secondary: `customer_id` + geospatial check (within 50m)\n- [ ] **Booking Matching**: By `crm_id` (unique constraint prevents duplicates)\n- [ ] **CRM ID Generation**:\n  ```\n  crm_id = SHA256(customer + \"|\" + location + \"|\" + date + \"|\" + driver)\n  Example: SHA256(\"DKW-Bojangles #542|1045 S Lake Blvd...|2026-01-01|Amari Dinkins\")\n  ```\n\n### 3. Update Logic\n- [ ] **Locations**: If exists, merge location requirements into `metadata` (don't overwrite verified coordinates or existing non-empty metadata values), add 'Imported' tag\n- [ ] **Bookings**: Skip if `crm_id` already exists\n\n### 4. Booking Generation\n- [ ] **Date Mapping**: `Job/Est Date` â†’ `bookings.scheduled_date`\n- [ ] **Status Mapping**:\n  | CSV Status | App Status | CRM Status | Notes |\n  |------------|------------|------------|-------|\n  | `Scheduled` | `confirmed` | SCHEDULED | Ready for routing |\n  | `Dispatched` | `scheduled` | DISPATCHED | Assigned but not sent to driver |\n  | `Closed and Complete` | `completed` | COMPLETED | Historical record |\n  | `Completed` | `completed` | COMPLETED | Historical record |\n- [ ] **Historical Bookings**: For `completed` status, also set `actual_start_time` = `scheduled_date`\n- [ ] **Service Type**: Always link to `GT-PUMP` service\n- [ ] **Driver Attribution**: For historical bookings, assign matched `driver_id` if available\n\n## Acceptance Criteria\n- [ ] Exactly 278 customers created (no duplicates on re-run)\n- [ ] Exactly 870 locations created, each linked to correct customer\n- [ ] All 1,193 bookings created with correct status and location linkage\n- [ ] Re-running import creates 0 new records (idempotent)\n- [ ] Spot check: \"Applebee's\" customer has exactly 30 locations\n- [ ] Spot check: \"Bojangles\" customer has exactly 222 locations\n- [ ] Locations tagged 'Needs Review' have clear issues logged\n\n## Dependencies\n- [ ] #15 Schema enhancements\n- [ ] #18 CSV Parser\n\n---\nðŸ“‹ Part of Data Import Modernization Plan - see `docs/data_import_modernization_plan.md`","labels":[{"id":"LA_kwDOQyPtOs8AAAACT1HZqg","name":"enhancement","description":"New feature or request","color":"a2eeef"},{"id":"LA_kwDOQyPtOs8AAAACVSanYg","name":"plan ready","description":"Planning complete - ready for implementation","color":"0052CC"},{"id":"LA_kwDOQyPtOs8AAAACVTMrzw","name":"plan: complex","description":"Complex (7+ pts) - needs requirements.md, design.md, tasks.md","color":"D93F0B"}],"number":19,"title":"feat: Database Upsert & Booking Generation for legacy import"},{"body":"## Goal\nParse and clean the legacy CSV data for import into Fleetillo.\n\n## Source Data\n- **File**: `docs/client_artifacts/ExpanedTechDaySheetReport_01_01_2026_03_31_2026.xlsx - Worksheet.csv`\n- **Records**: 1,193 rows (bookings/service visits)\n- **Unique Customers (Brands)**: 278\n- **Unique Locations (Sites)**: 870\n- **Drivers**: 12\n\n## Tasks\n\n### 1. Customer & Location Parsing Strategy\n\n**Critical Distinction:**\n- The CSV `Customer` column contains **LOCATION identifiers**, not customer names\n- The actual **Customer** is the BRAND extracted from this field\n- Multiple locations belong to a single customer\n\n- [ ] **Two-Step Parsing**:\n  ```\n  Step 1: Extract CUSTOMER (Brand)\n    1. Strip \"DKW-\" prefix\n    2. If \"#\" exists: Brand = text before \"#\"\n    3. Else if \" - \" exists: Brand = text before \" - \"\n    4. Else: Brand = full remaining string\n\n  Step 2: Extract LOCATION name\n    - Location Name = full string after \"DKW-\" prefix\n  ```\n\n- [ ] **Examples**:\n  | CSV Customer Field | â†’ Customer (Brand) | â†’ Location Name |\n  |--------------------|-------------------|-----------------|\n  | `DKW-Applebee's #1025138` | Applebee's | Applebee's #1025138 |\n  | `DKW-Applebee's #1130` | Applebee's | Applebee's #1130 |\n  | `DKW-BCS - College Park Elementary` | BCS | BCS - College Park Elementary |\n  | `DKW-Doc's BBQ` | Doc's BBQ | Doc's BBQ |\n\n- [ ] **Customer Deduplication**: Normalize brand names (trim, uppercase) before matching\n- [ ] **Location Uniqueness**: Unique by `customer_id` (FK) + `location_name` (normalized)\n\n### 2. Address Normalization\n- [ ] **Sanitization**: Remove `DKW-{...}` prefix if present in address string\n- [ ] **Validation Pipeline**:\n  1. Run Google Places TextSearch with raw address\n  2. Accept result if confidence > threshold\n  3. **Fallback**: Flag for manual review (Tag: 'Needs Review')\n- [ ] **Rate Limiting**: 200ms delay between geocoding calls\n\n### 3. Notes Field Extraction â†’ Location Requirements\n- [ ] **Regex Patterns** (extract into `locations.metadata`):\n  | Pattern | Target Field | Description |\n  |---------|--------------|-------------|\n  | `hose:(\\d+)` | `metadata.hose_length_req` | Equipment needed to reach trap |\n  | `Tanker:\\s*(Yes\\|No)` | `metadata.requires_tanker` | Whether tanker truck required |\n  | `ServiceTime:\\s*(.+?)(?=hose:\\|Tanker:\\|$)` | `metadata.preferred_service_time` | Access/timing constraints |\n- [ ] **Notes Preservation**: After extraction, store remaining text in `locations.notes`\n\n### 4. Gallons Field Parsing â†’ Location Capacity\n- [ ] **Standard**: Direct number â†’ `locations.metadata.capacity_gallons`\n- [ ] **Multi-Trap**: Pattern `(\\d+)-\\s*(\\d+)` (e.g., \"2- 2000\") â†’\n  ```json\n  { \"trap_count\": 2, \"capacity_gallons\": 2000 }\n  ```\n- [ ] **Complex Text**: Store raw value in `locations.metadata.capacity_notes` and add Tag: 'Needs Review'\n\n### 5. Driver Matching\n- [ ] Fuzzy match CSV \"Name\" column to `drivers.first_name + ' ' + drivers.last_name`\n- [ ] Store matched `driver_id` for historical booking attribution\n- [ ] Log warning if no match found\n\n## Acceptance Criteria\n- [ ] 278 unique customers (brands) extracted correctly\n- [ ] 870 unique locations parsed and linked to correct customer\n- [ ] All addresses normalized or flagged for review\n- [ ] Notes extraction produces valid metadata\n- [ ] Multi-trap gallons handled correctly\n\n## Dependencies\n- [ ] Can be developed in parallel with #15\n\n\n---\nðŸ“‹ Part of Data Import Modernization Plan - see `docs/data_import_modernization_plan.md`","labels":[{"id":"LA_kwDOQyPtOs8AAAACT1HZqg","name":"enhancement","description":"New feature or request","color":"a2eeef"},{"id":"LA_kwDOQyPtOs8AAAACVTMrzw","name":"plan: complex","description":"Complex (7+ pts) - needs requirements.md, design.md, tasks.md","color":"D93F0B"}],"number":18,"title":"feat: CSV Parser & Data Normalization for legacy import"},{"body":"## Goal\nSurface the new location data (tags, capacity, metadata) in the daily workflow views.\n\n## Tasks\n\n### Location List\n- [ ] Add column for \"Tags\" (display as pills/badges)\n- [ ] Add column for \"Capacity\" (e.g., \"1500 gal\")\n\n### Customer (Client) Details Page\n- [ ] Display metadata summary in the locations card (e.g., \"1500 gal / 4 wks\")\n- [ ] Show location tags\n\n## Acceptance Criteria\n- [ ] Location list shows Tags and Capacity columns\n- [ ] Customer detail page shows location metadata summary\n\n## Dependencies\n- [ ] #15 Schema enhancements\nCan be developed in parallel with #16\n\n---\nðŸ“‹ Part of Data Import Modernization Plan - see `docs/data_import_modernization_plan.md`","labels":[{"id":"LA_kwDOQyPtOs8AAAACT1HZqg","name":"enhancement","description":"New feature or request","color":"a2eeef"},{"id":"LA_kwDOQyPtOs8AAAACVTPbYw","name":"plan: simple","description":"Simple - no plan needed, ready to implement","color":"0E8A16"}],"number":17,"title":"feat: Enhanced Location Lists & Customer Detail Views"},{"body":"## Goal\nAllow operators to view and edit the new location fields (tags, metadata for site-specific requirements).\n\n## Tasks\n\n### Create/Edit Location Form\n- [ ] **Customer Labeling**: Ensure UI refers to \"Clients\" table as \"Customers\"\n- [ ] **Tags Input**: Pill selector for `tags`\n- [ ] **Location Requirements Editor** (stored in `metadata` JSONB):\n  - **Standard Fields**: Explicit inputs for:\n    - `Capacity (Gallons)` - trap size at this location\n    - `Trap Count` - number of traps\n    - `Service Frequency (Weeks)` - how often this location needs service\n  - **Access Requirements**:\n    - `Hose Length Required` - equipment needed to reach trap\n    - `Requires Tanker` - checkbox\n    - `Preferred Service Time` - access/timing constraints\n  - **Flexible Attributes**: Key-value editor for other location-specific requirements\n\n### Address Validation\n- [ ] Ensure the form uses Google Places Autocomplete to save standardized addresses and coordinates\n\n## Acceptance Criteria\n- [ ] Can create location with tags and metadata\n- [ ] Metadata persists correctly in Supabase\n- [ ] Address autocomplete populates lat/lon\n\n## Dependencies\n- [ ] #15 Schema enhancements\n\n---\nðŸ“‹ Part of Data Import Modernization Plan - see `docs/data_import_modernization_plan.md`","labels":[{"id":"LA_kwDOQyPtOs8AAAACT1HZqg","name":"enhancement","description":"New feature or request","color":"a2eeef"},{"id":"LA_kwDOQyPtOs8AAAACVSanYg","name":"plan ready","description":"Planning complete - ready for implementation","color":"0052CC"},{"id":"LA_kwDOQyPtOs8AAAACVTMrpw","name":"plan: medium","description":"Medium (3-6 pts) - needs plan.md document","color":"FBCA04"}],"number":16,"title":"feat: Update Location Management Forms for metadata and tags"},{"body":"## Goal\nEnable flexible attribute storage, legacy CRM tracking, and ensure service type exists.\n\n## Tasks\n\n### Migration: Add columns to `locations` table\n- [ ] `tags` (text[], default `{}`) - For labeling (e.g., 'Imported', 'Needs Review')\n- [ ] `metadata` (jsonb, default `{}`) - For **location-specific requirements**, including:\n  - `capacity_gallons` (number) - Grease trap capacity at this location\n  - `trap_count` (number) - Number of traps at this location\n  - `service_frequency_weeks` (number) - How often this location needs service\n  - `hose_length_req` (string) - Hose length required to reach the trap\n  - `requires_tanker` (boolean) - Whether a tanker truck is needed\n  - `preferred_service_time` (string) - Access/timing constraints\n  - `capacity_notes` (string) - Complex capacity descriptions\n\n### Migration: Add columns to `bookings` table\n- [ ] `crm_status` (text) - To track the external CRM status (e.g., \"SCHEDULED\", \"COMPLETED\")\n- [ ] `crm_id` (text, unique) - External Reference ID to prevent duplicate imports\n\n### Service Setup\n- [ ] Verify/Create \"GT-PUMP\" service (name, code, average duration)\n  - *Note: The service type only defines WHAT work is done. Site-specific requirements are stored on each Location.*\n\n### Drivers Setup\n- [ ] Verify `drivers` table exists with `first_name`, `last_name`, `status` fields\n- [ ] Pre-create the 12 drivers from CSV if they don't exist:\n  - Amari Dinkins, Jiro Prioleau, William Emerson, Jamel Lloyd\n  - John Elledge, Travis Menius, CHRLS Route, Upstate Route 2\n  - OuterBanks Route, MB Route, Augusta Route, WNC Route\n\n## Acceptance Criteria\n- [ ] Migration runs without errors\n- [ ] `npm run db:check` passes\n- [ ] GT-PUMP service exists in database\n- [ ] All 12 drivers exist in drivers table\n\n## Dependencies\n- [ ] #14 Rename clients â†’ customers\n\n---\nðŸ“‹ Part of Data Import Modernization Plan - see `docs/data_import_modernization_plan.md`","labels":[{"id":"LA_kwDOQyPtOs8AAAACT1HZqg","name":"enhancement","description":"New feature or request","color":"a2eeef"},{"id":"LA_kwDOQyPtOs8AAAACVTPbYw","name":"plan: simple","description":"Simple - no plan needed, ready to implement","color":"0E8A16"}],"number":15,"title":"feat: Schema enhancements for data import (metadata, tags, CRM tracking)"},{"body":"## Enhancement Description\n\nImplement an automated workflow that monitors DigitalOcean App Platform runtime logs to proactively detect errors, anomalies, and potential bugs before they're reported by users.\n\n## Motivation\n\nDuring a recent debugging session, critical information about a dispatch failure was found in the DigitalOcean runtime logs - including the exact sequence of events that caused the bug:\n- Telegram registration success followed by form submission that wiped the data\n- 302 redirect errors from Telegram webhook\n- Invalid API key errors for email fallback\n\nThis information was only discovered through manual log inspection. An automated monitoring system could have detected these issues immediately.\n\n## Proposed Features\n\n### 1. Log Monitoring Agent\n- Periodically fetch runtime logs from DigitalOcean using the MCP integration\n- Parse structured JSON logs from the dispatch service\n- Identify error patterns and anomalies\n\n### 2. Error Pattern Detection\n- `level: \"error\"` - Any error-level log entries\n- `level: \"warn\"` with high frequency - Repeated warnings\n- HTTP 4xx/5xx responses in sequence\n- Known error signatures (e.g., \"chat not found\", \"API key is invalid\")\n\n### 3. Automated Actions\n- **Alert**: Send notification (Slack, email, or in-app) when errors detected\n- **Issue Creation**: Automatically create GitHub issues for recurring or critical errors\n- **Correlation**: Link related errors together (e.g., dispatch failure â†’ telegram error â†’ email fallback error)\n\n### 4. Log Retention & Analysis\n- Store parsed log summaries in Supabase for historical analysis\n- Track error frequency over time\n- Identify regression patterns after deployments\n\n## Technical Approach\n\n### Option A: Scheduled Job\nAdd a new DigitalOcean scheduled job (like dispatch-poller) that:\n1. Calls `doctl apps logs` or DigitalOcean API\n2. Parses and analyzes log content\n3. Stores findings and triggers alerts\n\n### Option B: Log Forwarding\nConfigure DigitalOcean log forwarding to:\n1. External log aggregator (Datadog, Papertrail, etc.)\n2. Custom webhook endpoint for processing\n3. Supabase edge function for real-time analysis\n\n### Option C: AI-Assisted Analysis\nUse the existing AI assistant infrastructure to:\n1. Periodically review logs via MCP integration\n2. Apply pattern recognition for anomaly detection\n3. Generate human-readable summaries and recommendations\n\n## Acceptance Criteria\n\n- [ ] Errors are detected within 15 minutes of occurrence\n- [ ] Critical errors trigger immediate notifications\n- [ ] Recurring errors auto-create GitHub issues with log context\n- [ ] Dashboard shows error trends and health status\n- [ ] False positive rate is manageable (< 10%)\n\n## Related\n\n- DigitalOcean MCP integration already available\n- Current log sources: web service, dispatch-poller, end-of-day jobs\n- Issue #12 could have been detected automatically with this system","labels":[{"id":"LA_kwDOQyPtOs8AAAACT1HZqg","name":"enhancement","description":"New feature or request","color":"a2eeef"},{"id":"LA_kwDOQyPtOs8AAAACVTMrzw","name":"plan: complex","description":"Complex (7+ pts) - needs requirements.md, design.md, tasks.md","color":"D93F0B"}],"number":13,"title":"Automated DigitalOcean log monitoring for proactive bug detection"},{"body":"## Summary\n\nEnhance the OptiRoute AI Assistant (gradient-agents integration) to detect when users are reporting bugs, requesting help with errors, or suggesting enhancements, and automatically create GitHub issues on their behalf with status tracking.\n\n## User Story\n\nAs a user interacting with the OptiRoute Assistant, I want to be able to describe a problem, bug, or feature request in natural language, and have the assistant:\n1. Recognize my intent to submit a support request\n2. Gather necessary details through conversation\n3. Create a well-formatted GitHub issue automatically\n4. Provide me with status updates on my submitted issues\n\n## Proposed Functionality\n\n### Intent Detection\nThe assistant should recognize phrases like:\n- \"I found a bug...\"\n- \"This isn't working correctly...\"\n- \"Can you submit this as an issue?\"\n- \"I'd like to request a feature...\"\n- \"There's an error when I...\"\n\n### Issue Creation Flow\n1. **Classify the request**: Bug, Error, or Enhancement\n2. **Gather details**:\n   - Clear description of the issue/request\n   - Steps to reproduce (for bugs)\n   - Expected vs actual behavior\n   - Screenshots or context if available\n3. **Draft the issue**: Show user a preview before submitting\n4. **Create via GitHub API**: Submit to the optiroute repository\n5. **Confirm**: Provide issue link and number to user\n\n### Status Updates\n- User can ask: \"What's the status of my issue #X?\"\n- Assistant retrieves issue state, comments, and labels\n- Summarizes progress in conversational format\n\n## Technical Considerations\n\n- Requires GitHub API integration (personal access token or GitHub App)\n- May need to extend the assistant's tool capabilities\n- Consider rate limiting and authentication handling\n- Store mapping of user conversations to created issues\n\n## Acceptance Criteria\n\n- [ ] Assistant can detect support request intent\n- [ ] Assistant can create issues with appropriate labels (bug, enhancement, etc.)\n- [ ] Assistant shows draft before submitting\n- [ ] Assistant provides issue link after creation\n- [ ] Assistant can query and report issue status\n- [ ] Issues are well-formatted with all relevant context\n\n## Related\n\n- Current assistant implementation in `web-launcher/` integration with gradient-agents","labels":[{"id":"LA_kwDOQyPtOs8AAAACT1HZqg","name":"enhancement","description":"New feature or request","color":"a2eeef"},{"id":"LA_kwDOQyPtOs8AAAACVSanYg","name":"plan ready","description":"Planning complete - ready for implementation","color":"0052CC"},{"id":"LA_kwDOQyPtOs8AAAACVTMrzw","name":"plan: complex","description":"Complex (7+ pts) - needs requirements.md, design.md, tasks.md","color":"D93F0B"}],"number":10,"title":"AI Assistant: Enable support request handling via GitHub Issues"},{"body":"## Summary\n\nThe \"Traffic Awareness\" option in the Plan Routes modal shows incorrectly for future dates due to a UTC vs local time bug. Additionally, research is needed on how traffic-aware routing should work with overnight dispatch scheduling.\n\n## Current Bug\n\nThe `updateTrafficOptions()` function in `web-launcher/public/routes.html` uses UTC time comparison instead of local time:\n\n```javascript\nconst today = new Date().toISOString().split('T')[0];  // Returns UTC date!\n```\n\n**Example:** At local time `2026-01-18T19:56:28-06:00` (CST):\n- `new Date().toISOString()` returns `2026-01-19T01:56:28Z` (UTC)\n- So \"today\" becomes `2026-01-19` instead of `2026-01-18`\n\n**Result:** Traffic Awareness shows for January 19th (tomorrow) because it matches the UTC \"today\".\n\n## Research Needed\n\nThe label states \"Traffic Awareness (only for same-day routes)\" - but consider:\n\n1. **Dispatch jobs run overnight**: Routes are typically planned and dispatched the night before the scheduled route date\n2. **Traffic data relevance**: Real-time traffic data is only useful for routes happening \"right now\"\n3. **Questions to answer**:\n   - Should traffic awareness be available for next-day routes when dispatching the night before?\n   - Should we re-optimize routes in the morning when traffic data is current?\n   - Is the Google Routes API's traffic data predictive for future dates, or only real-time?\n\n## Proposed Fix (for the bug)\n\nReplace UTC comparison with local date:\n```javascript\nconst now = new Date();\nconst today = `${now.getFullYear()}-${String(now.getMonth() + 1).padStart(2, '0')}-${String(now.getDate()).padStart(2, '0')}`;\n```\n\n## Files Affected\n\n- `web-launcher/public/routes.html` - Lines 1285-1295 (`updateTrafficOptions` function)\n\n## Labels\n\nbug, enhancement, needs-research","labels":[{"id":"LA_kwDOQyPtOs8AAAACT1HZpg","name":"bug","description":"Something isn't working","color":"d73a4a"},{"id":"LA_kwDOQyPtOs8AAAACVSanYg","name":"plan ready","description":"Planning complete - ready for implementation","color":"0052CC"},{"id":"LA_kwDOQyPtOs8AAAACVTPbYw","name":"plan: simple","description":"Simple - no plan needed, ready to implement","color":"0E8A16"}],"number":9,"title":"Traffic Awareness: Fix UTC date comparison bug and research dispatch scheduling implications"},{"body":"To prepare for production deployment a full security audit is necessary to determine what tasks should be created to address critical and moderate security vulnurabilities.","labels":[{"id":"LA_kwDOQyPtOs8AAAACVSanYg","name":"plan ready","description":"Planning complete - ready for implementation","color":"0052CC"},{"id":"LA_kwDOQyPtOs8AAAACVTMrzw","name":"plan: complex","description":"Complex (7+ pts) - needs requirements.md, design.md, tasks.md","color":"D93F0B"}],"number":8,"title":"Security Audit January 2026"},{"body":"I need a way to geofence vehicles so that when they are assigned to a route, they are constrained by their geofence.","labels":[{"id":"LA_kwDOQyPtOs8AAAACT1HZqg","name":"enhancement","description":"New feature or request","color":"a2eeef"},{"id":"LA_kwDOQyPtOs8AAAACVTMrzw","name":"plan: complex","description":"Complex (7+ pts) - needs requirements.md, design.md, tasks.md","color":"D93F0B"}],"number":3,"title":"vehicle geofencing"},{"body":"I need a telegram messaging template system so that I can update the text that is sent for the dispatches.","labels":[{"id":"LA_kwDOQyPtOs8AAAACT1HZqg","name":"enhancement","description":"New feature or request","color":"a2eeef"},{"id":"LA_kwDOQyPtOs8AAAACVTMrzw","name":"plan: complex","description":"Complex (7+ pts) - needs requirements.md, design.md, tasks.md","color":"D93F0B"}],"number":2,"title":"messaging templates"}]
